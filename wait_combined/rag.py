import ollama
import os
from langchain_community.document_loaders import PyPDFLoader
from langchain_ollama import OllamaEmbeddings 
from langchain_chroma import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter

# ------------------ è¼‰å…¥ PDF æ•™æä¸¦å»ºç«‹å‘é‡è³‡æ–™åº« ------------------

PDF_PATH = "Abraham Silberschatz Operating System Concepts.pdf"  # â† æ”¾ä½ çš„æ•™æ
DB_DIR = "./db"

if not os.path.exists(DB_DIR):
    print("ğŸ§  ç¬¬ä¸€æ¬¡åŸ·è¡Œï¼šè¼‰å…¥ PDF ä¸¦å»ºç«‹å‘é‡è³‡æ–™åº«...")
    loader = PyPDFLoader(PDF_PATH)
    documents = loader.load()

    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    docs = splitter.split_documents(documents)

    embedding = OllamaEmbeddings(model="nomic-embed-text")  # è¨˜å¾—å…ˆ ollama pull nomic-embed-text
    vectordb = Chroma.from_documents(docs, embedding, persist_directory=DB_DIR)
    # âŒ ä¸éœ€è¦ vectordb.persist()ï¼Œæ–°ç‰ˆæœƒè‡ªå‹•å„²å­˜
else:
    print("âœ… å·²è¼‰å…¥æœ¬åœ°è³‡æ–™åº«")
    embedding = OllamaEmbeddings(model="nomic-embed-text")
    vectordb = Chroma(persist_directory=DB_DIR, embedding_function=embedding)

retriever = vectordb.as_retriever()

# ------------------ LLM è¨Šæ¯è™•ç† ------------------

messages = [{"role": "system", "content": "ä½ æ˜¯ä¸€ä½ç”¨ç¹é«”ä¸­æ–‡å›ç­”çš„å®¶æ•™è€å¸«ï¼Œè«‹ç”¨ç°¡å–®æ–¹å¼è¬›è§£å•é¡Œã€‚"}]
MODEL_NAME = 'llama3'

def chat_with_ollama(prompt):
    # ğŸ” æª¢ç´¢ç›¸é—œå…§å®¹ï¼ˆRAGï¼‰
    related_docs = retriever.invoke(prompt)
    context_text = "\n---\n".join([doc.page_content for doc in related_docs[:3]])  # å–å‰3ç­†è³‡æ–™

    # åŒ…è£ prompt åŠ å…¥ä¸Šä¸‹æ–‡
    rag_prompt = f"""ä»¥ä¸‹æ˜¯æ•™æå…§å®¹æ‘˜è¦ï¼Œè«‹æ ¹æ“šé€™äº›å…§å®¹ä¾†å›ç­”å•é¡Œï¼š

    {context_text}

    ä½¿ç”¨è€…å•é¡Œï¼š{prompt}
    è«‹ç”¨ç¹é«”ä¸­æ–‡è©³ç´°è§£é‡‹ï¼Œä¸¦èˆ‰ä¾‹å­èªªæ˜ã€‚"""

    messages.append({"role": "user", "content": rag_prompt})

    try:
        response = ollama.chat(model=MODEL_NAME, messages=messages)
        reply = response['message']['content']
        messages.append({"role": "assistant", "content": reply})

        # æ§åˆ¶è¨Šæ¯å †ç©ï¼ˆç°¡æ˜“è¨˜æ†¶ï¼‰
        if len(messages) > 20:
            messages.pop(1)

        return reply
    except Exception as e:
        return f"âŒ ç™¼ç”ŸéŒ¯èª¤ï¼š{e}"

# ------------------ ä¸»ç¨‹å¼ ------------------

if __name__ == "__main__":
    print("ğŸ“˜ RAG å®¶æ•™ç³»çµ±å·²å•Ÿå‹•ï¼Œè¼¸å…¥ quit é›¢é–‹")

    while True:
        user_input = input("ä½ ï¼š").strip()
        
        if not user_input:
            continue

        if user_input.lower() in ["quit", "exit", "bye"]:
            print("ğŸ‘‹ å†è¦‹ï¼Œç¥å­¸ç¿’æ„‰å¿«ï¼")
            break

        response = chat_with_ollama(user_input)
        print("å®¶æ•™è€å¸«ï¼š", response)
