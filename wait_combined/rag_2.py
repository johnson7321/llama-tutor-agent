import ollama
import os
import json
from datetime import datetime
from openpyxl import Workbook, load_workbook
from collections import defaultdict
from langchain_community.document_loaders import PyPDFLoader
from langchain_ollama import OllamaEmbeddings
from langchain_chroma import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter

# ------------------ æ•™æä¾†æº ------------------

PDF_LIST = [
    "Abraham Silberschatz Operating System Concepts.pdf",
    "Computer Organization and Design.pdf"
]

DB_DIR = "./db"
DB_INFO_FILE = os.path.join(DB_DIR, "dbinfo.json")

# ------------------ æª¢æŸ¥æ˜¯å¦æœ‰æ–°æ•™æ ------------------

# è®€å–ä¹‹å‰å·²ç¶“åŒ¯å…¥çš„æ•™ææª”åæ¸…å–®
def get_loaded_pdf_list():
    if os.path.exists(DB_INFO_FILE):
        with open(DB_INFO_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return []

# å„²å­˜ç›®å‰æ•™ææ¸…å–®ï¼Œä»¥ä¾¿ä¸‹æ¬¡æ¯”å°æ˜¯å¦æœ‰æ–°å¢æˆ–ç•°å‹•
def save_loaded_pdf_list(pdf_list):
    os.makedirs(DB_DIR, exist_ok=True)
    with open(DB_INFO_FILE, "w", encoding="utf-8") as f:
        json.dump(pdf_list, f, ensure_ascii=False, indent=2)

# ------------------ å»ºç«‹çŸ¥è­˜åº« ------------------

# æ¯”å°æ•™ææ¸…å–®ï¼Œç¢ºèªæ˜¯å¦æœ‰ç•°å‹•éœ€è¦æ›´æ–°çŸ¥è­˜åº«
need_refresh = sorted(PDF_LIST) != sorted(get_loaded_pdf_list())

if need_refresh:
    print("ğŸ§  åµæ¸¬åˆ°æœ‰æ–°æ•™æè®Šå‹•ï¼Œé–‹å§‹é‡æ–°å»ºç«‹å‘é‡è³‡æ–™åº«...")
    documents = []

    for pdf_file in PDF_LIST:
        if os.path.exists(pdf_file):
            print(f"ğŸ“˜ è¼‰å…¥æ•™æï¼š{pdf_file}")
            loader = PyPDFLoader(pdf_file)
            loaded_docs = loader.load()
            for doc in loaded_docs:
                doc.metadata["source_file"] = pdf_file  # åŠ å…¥ä¾†æºæª”åä»¥ä¾¿æŸ¥è©¢
            documents.extend(loaded_docs)
        else:
            print(f"âš ï¸ æ‰¾ä¸åˆ°æ•™æï¼š{pdf_file}")

    # åˆ†å‰²æ®µè½å…§å®¹ä¾›å‘é‡ç´¢å¼•ä½¿ç”¨
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    docs = splitter.split_documents(documents)

    # å»ºç«‹å‘é‡è³‡æ–™åº«
    embedding = OllamaEmbeddings(model="nomic-embed-text")
    vectordb = Chroma.from_documents(docs, embedding, persist_directory=DB_DIR)
    save_loaded_pdf_list(PDF_LIST)

    # ğŸ“š ç”¢ç”Ÿç« ç¯€æ‘˜è¦
    def generate_textbook_outline(docs):
        outlines = defaultdict(list)
        for doc in docs:
            source = doc.metadata.get("source_file", "æœªçŸ¥æ•™æ")
            content = doc.page_content
            lines = content.splitlines()
            for line in lines:
                line = line.strip()
                if line.lower().startswith("chapter") or line[:3].isdigit():
                    if len(line) < 100:
                        outlines[source].append(line)
        return outlines

    outlines = generate_textbook_outline(documents)
    print("\nğŸ“š æ•™æç« ç¯€ç›®éŒ„æ‘˜è¦ï¼š")
    for src, chapters in outlines.items():
        print(f"\nğŸ“˜ {src}")
        for ch in chapters[:10]:
            print(f"  - {ch}")
else:
    print("âœ… æ•™ææœªè®Šå‹•ï¼Œç›´æ¥è¼‰å…¥è³‡æ–™åº«")
    embedding = OllamaEmbeddings(model="nomic-embed-text")
    vectordb = Chroma(persist_directory=DB_DIR, embedding_function=embedding)

# å»ºç«‹æª¢ç´¢å™¨
retriever = vectordb.as_retriever()

# ------------------ å„²å­˜å°è©±ç´€éŒ„ ------------------

# å„²å­˜å°è©±ç‚º Excelï¼Œé¿å…ä¸­æ–‡äº‚ç¢¼
def save_chat_xlsx(user_msg, bot_reply):
    today_str = datetime.now().strftime("%Y-%m-%d")
    filename = f"chat_log_{today_str}.xlsx"
    time_str = datetime.now().strftime("%H:%M:%S")

    try:
        if os.path.exists(filename):
            wb = load_workbook(filename)
            ws = wb.active
        else:
            wb = Workbook()
            ws = wb.active
            ws.append(["æ™‚é–“", "ä½¿ç”¨è€…", "AI å›è¦†"])

        ws.append([time_str, user_msg, bot_reply])
        wb.save(filename)
    except PermissionError:
        print("âš ï¸ æª”æ¡ˆå¯èƒ½å·²é–‹å•Ÿä¸­ï¼Œè«‹é—œé–‰ Excel å¾Œé‡è©¦ã€‚")

# ------------------ LLM å°è©±è™•ç† ------------------

# åˆå§‹ prompt è¨Šæ¯
messages = [{"role": "system", "content": "ä½ æ˜¯ä¸€ä½ç”¨ç¹é«”ä¸­æ–‡å›ç­”çš„å®¶æ•™è€å¸«ï¼Œè«‹ç”¨ç°¡å–®æ–¹å¼è¬›è§£å•é¡Œã€‚"}]
MODEL_NAME = 'llama3'
last_user = ""
last_reply = ""

# å•ç­”ä¸»é‚è¼¯å‡½å¼

def chat_with_ollama(prompt):
    global last_user, last_reply

    # å˜—è©¦å¾ prompt åˆ¤æ–·æ˜¯å¦æœ‰æŒ‡å®šæŸ¥è©¢æŸæœ¬æ•™æ
    selected_pdf = None
    for pdf_file in PDF_LIST:
        base = os.path.splitext(os.path.basename(pdf_file))[0].lower()
        if base in prompt.lower():
            selected_pdf = pdf_file
            break

    # åŸ·è¡Œæª¢ç´¢ï¼Œä¸¦æ ¹æ“šæŒ‡å®šæ•™æéæ¿¾
    if selected_pdf:
        print(f"ğŸ¯ åªæœå°‹æ•™æï¼š{selected_pdf}")
        all_docs = retriever.invoke(prompt)
        related_docs = [doc for doc in all_docs if doc.metadata.get("source_file") == selected_pdf][:3]
        if not related_docs:
            related_docs = all_docs[:3]  # è‹¥æŒ‡å®šçµæœå¤ªå°‘å‰‡é€€å›å…¨éƒ¨
    else:
        related_docs = retriever.invoke(prompt)[:3]

    # å»ºç«‹ä¸Šä¸‹æ–‡å…§å®¹ï¼ˆå«ä¾†æºèˆ‡é æ•¸ï¼‰
    context_chunks = []
    for doc in related_docs:
        source = doc.metadata.get("source_file", "æœªçŸ¥æ•™æ")
        page = doc.metadata.get("page", "æœªçŸ¥é æ•¸")
        chunk = f"[ä¾†è‡ªï¼š{source} ç¬¬ {page} é ]\n{doc.page_content}"
        context_chunks.append(chunk)

    context_text = "\n---\n".join(context_chunks)

    # åŒ…è£ prompt çµ¦æ¨¡å‹
    rag_prompt = f"""ä»¥ä¸‹æ˜¯æ•™æå…§å®¹æ‘˜è¦ï¼Œè«‹æ ¹æ“šé€™äº›å…§å®¹ä¾†å›ç­”å•é¡Œï¼š\n\n{context_text}\n\nä½¿ç”¨è€…å•é¡Œï¼š{prompt}\nè«‹ç”¨ç¹é«”ä¸­æ–‡è©³ç´°è§£é‡‹ï¼Œä¸¦èˆ‰ä¾‹å­èªªæ˜ã€‚"""

    messages.append({"role": "user", "content": rag_prompt})

    try:
        response = ollama.chat(model=MODEL_NAME, messages=messages)
        reply = response['message']['content']
        messages.append({"role": "assistant", "content": reply})

        # å„²å­˜å°è©±ç´€éŒ„
        save_chat_xlsx(prompt, reply)

        # è¨˜ä½é€™æ¬¡å°è©±
        last_user = prompt
        last_reply = reply

        # æ§åˆ¶è¨Šæ¯å †ç©ä¸Šé™
        if len(messages) > 20:
            messages.pop(1)

        return reply
    except Exception as e:
        return f"âŒ ç™¼ç”ŸéŒ¯èª¤ï¼š{e}"

# ------------------ ä¸»ç¨‹å¼ ------------------

if __name__ == "__main__":
    print("ğŸ“˜ å®¶æ•™ç³»çµ±å·²å•Ÿå‹•ï¼Œè¼¸å…¥ quit,exit,bye é›¢é–‹")

    while True:
        user_input = input("ä½ ï¼š").strip()

        if not user_input:
            continue

        if user_input.lower() in ["quit", "exit", "bye"]:
            print("ğŸ‘‹ å†è¦‹ï¼Œç¥å­¸ç¿’æ„‰å¿«ï¼")
            break

        response = chat_with_ollama(user_input)
        print("å®¶æ•™è€å¸«ï¼š", response)
