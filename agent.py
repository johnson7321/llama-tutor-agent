import ollama
import prompt
import message_history_txt

def chat_with_llama():

    
    messages = []  # ç”¨ä¾†å„²å­˜å°è©±æ­·å²ï¼Œä»¥ä¾¿ç¶­æŒä¸Šä¸‹æ–‡
    
    messages.append({"role": "user", "content": prompt.tutor_guideline})
    messages.append({"role": "user", "content": prompt.teaching_quiz})
    messages.append({"role": "user", "content": prompt.Progress})
    messages.append({"role": "user", "content": message_history_txt.message_history})


    while True:
        try:
            user_input = input("ğŸ‘¤ ä½ : ")  # æ¥æ”¶ä½¿ç”¨è€…è¼¸å…¥
            
            if user_input.lower() == "exit":  # è‹¥ä½¿ç”¨è€…è¼¸å…¥ 'exit'ï¼Œå‰‡çµæŸå°è©±
                print("ğŸ”´ çµæŸå°è©±...\n")
                break
            
            messages.append({"role": "user", "content": user_input})  # å„²å­˜ä½¿ç”¨è€…è¼¸å…¥
            
            response = ollama.chat(model='llama3.1', messages=messages)  # å‘ Llama 3.1  ç™¼é€è«‹æ±‚
            
            bot_reply = response.get('message', {}).get('content', 'âš ï¸ ç„¡æ³•ç²å–å›æ‡‰')  # å–å¾—å›æ‡‰å…§å®¹ï¼Œä¸¦è™•ç†å¯èƒ½çš„éŒ¯èª¤
            
            messages.append({"role": "assistant", "content": bot_reply})  # å„²å­˜ AI çš„å›æ‡‰
            
            
            print("ğŸ¤– Llama: "+bot_reply)  # é¡¯ç¤º AI çš„å›æ‡‰

        
        except Exception as e:
            print(f"âš ï¸ ç™¼ç”ŸéŒ¯èª¤: {e}\n")  # æ•æ‰ä¸¦é¡¯ç¤ºéŒ¯èª¤è¨Šæ¯

if __name__ == "__main__":
    chat_with_llama()  # åŸ·è¡Œå°è©±å‡½å¼
